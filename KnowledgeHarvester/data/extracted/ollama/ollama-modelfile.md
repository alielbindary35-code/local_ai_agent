Modelfile Reference - Ollama[Skip to main content](#content-area)[Ollama home page![light logo](https://mintcdn.com/ollama-9269c548/XefrxzvUktkk84RL/images/logo.png?fit=max&auto=format&n=XefrxzvUktkk84RL&q=85&s=03a38b5749aa7f530044e9b251dc10d8)![dark logo](https://mintcdn.com/ollama-9269c548/XefrxzvUktkk84RL/images/logo-dark.png?fit=max&auto=format&n=XefrxzvUktkk84RL&q=85&s=c214b467f5623414c31d4e05c66110fb)](https://ollama.com)Search...⌘K
##### Get started

* [Welcome](/)
* [Quickstart](/quickstart)
* [Cloud](/cloud)
##### Capabilities

* [Streaming](/capabilities/streaming)
* [Thinking](/capabilities/thinking)
* [Structured Outputs](/capabilities/structured-outputs)
* [Vision](/capabilities/vision)
* [Embeddings](/capabilities/embeddings)
* [Tool calling](/capabilities/tool-calling)
* [Web search](/capabilities/web-search)
##### Integrations

* [VS Code](/integrations/vscode)
* [JetBrains](/integrations/jetbrains)
* [Codex](/integrations/codex)
* [Cline](/integrations/cline)
* [Droid](/integrations/droid)
* [Goose](/integrations/goose)
* [Zed](/integrations/zed)
* [Roo Code](/integrations/roo-code)
* [n8n](/integrations/n8n)
* [Xcode](/integrations/xcode)
##### More information

* [CLI Reference](/cli)
* [Modelfile Reference](/modelfile)
* [Context length](/context-length)
* [Linux](/linux)
* [macOS](/macos)
* [Windows](/windows)
* [Docker](/docker)
* [Importing a Model](/import)
* [FAQ](/faq)
* [Hardware support](/gpu)
* [Troubleshooting](/troubleshooting)
* [Sign in](https://ollama.com/signin)
* [Download](https://ollama.com/download)
* 
[Ollama home page![light logo](https://mintcdn.com/ollama-9269c548/XefrxzvUktkk84RL/images/logo.png?fit=max&auto=format&n=XefrxzvUktkk84RL&q=85&s=03a38b5749aa7f530044e9b251dc10d8)![dark logo](https://mintcdn.com/ollama-9269c548/XefrxzvUktkk84RL/images/logo-dark.png?fit=max&auto=format&n=XefrxzvUktkk84RL&q=85&s=c214b467f5623414c31d4e05c66110fb)](https://ollama.com)Search...⌘KSearch...NavigationMore informationModelfile Reference[Documentation](/)[API Reference](/api/introduction)[Documentation](/)[API Reference](/api/introduction)More information# Modelfile Reference

Copy pageCopy pageA Modelfile is the blueprint to create and share customized models using Ollama.
## [​](#table-of-contents)Table of Contents


* [Format](#format)
* [Examples](#examples)
* [Instructions](#instructions)
	+ [FROM (Required)](#from-required)
		- [Build from existing model](#build-from-existing-model)
		- [Build from a Safetensors model](#build-from-a-safetensors-model)
		- [Build from a GGUF file](#build-from-a-gguf-file)
	+ [PARAMETER](#parameter)
		- [Valid Parameters and Values](#valid-parameters-and-values)
	+ [TEMPLATE](#template)
		- [Template Variables](#template-variables)
	+ [SYSTEM](#system)
	+ [ADAPTER](#adapter)
	+ [LICENSE](#license)
	+ [MESSAGE](#message)
* [Notes](#notes)


## [​](#format)Format


The format of the `Modelfile`:
Copy
```
# comment
INSTRUCTION arguments

```



| Instruction | Description |
| --- | --- |
| [`FROM`](#from-required) (required) | Defines the base model to use. |
| [`PARAMETER`](#parameter) | Sets the parameters for how Ollama will run the model. |
| [`TEMPLATE`](#template) | The full prompt template to be sent to the model. |
| [`SYSTEM`](#system) | Specifies the system message that will be set in the template. |
| [`ADAPTER`](#adapter) | Defines the (Q)LoRA adapters to apply to the model. |
| [`LICENSE`](#license) | Specifies the legal license. |
| [`MESSAGE`](#message) | Specify message history. |


## [​](#examples)Examples


### [​](#basic-modelfile)Basic `Modelfile`


An example of a `Modelfile` creating a mario blueprint:
Copy
```
FROM llama3.2
# sets the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1
# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token
PARAMETER num\_ctx 4096

# sets a custom system message to specify the behavior of the chat assistant
SYSTEM You are Mario from super mario bros, acting as an assistant.

```

To use this:
1. Save it as a file (e.g. `Modelfile`)
2. `ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>`
3. `ollama run choose-a-model-name`
4. Start using the model!


To view the Modelfile of a given model, use the `ollama show --modelfile` command.
Copy
```
ollama show --modelfile llama3.2

```

Copy
```
# Modelfile generated by "ollama show"
# To build a new Modelfile based on this one, replace the FROM line with:
# FROM llama3.2:latest
FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29
TEMPLATE """{{ if .System }}<|start\_header\_id|>system<|end\_header\_id|>

{{ .System }}<|eot\_id|>{{ end }}{{ if .Prompt }}<|start\_header\_id|>user<|end\_header\_id|>

{{ .Prompt }}<|eot\_id|>{{ end }}<|start\_header\_id|>assistant<|end\_header\_id|>

{{ .Response }}<|eot\_id|>"""
PARAMETER stop "<|start\_header\_id|>"
PARAMETER stop "<|end\_header\_id|>"
PARAMETER stop "<|eot\_id|>"
PARAMETER stop "<|reserved\_special\_token"

```

## [​](#instructions)Instructions


### [​](#from-required)FROM (Required)


The `FROM` instruction defines the base model to use when creating a model.
Copy
```
FROM <model name>:<tag>

```

#### [​](#build-from-existing-model)Build from existing model


Copy
```
FROM llama3.2

```

[## Base Models

A list of available base models](https://github.com/ollama/ollama#model-library)
[## Base Models

Additional models can be found at](https://ollama.com/library)
#### [​](#build-from-a-safetensors-model)Build from a Safetensors model


Copy
```
FROM <model directory>

```

The model directory should contain the Safetensors weights for a supported architecture.
Currently supported model architectures:
* Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2)
* Mistral (including Mistral 1, Mistral 2, and Mixtral)
* Gemma (including Gemma 1 and Gemma 2)
* Phi3


#### [​](#build-from-a-gguf-file)Build from a GGUF file


Copy
```
FROM ./ollama-model.gguf

```

The GGUF file location should be specified as an absolute path or relative to the `Modelfile` location.
### [​](#parameter)PARAMETER


The `PARAMETER` instruction defines a parameter that can be set when the model is run.
Copy
```
PARAMETER <parameter> <parametervalue>

```

#### [​](#valid-parameters-and-values)Valid Parameters and Values




| Parameter | Description | Value Type | Example Usage |
| --- | --- | --- | --- |
| num\_ctx | Sets the size of the context window used to generate the next token. (Default: 2048) | int | num\_ctx 4096 |
| repeat\_last\_n | Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num\_ctx) | int | repeat\_last\_n 64 |
| repeat\_penalty | Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1) | float | repeat\_penalty 1.1 |
| temperature | The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8) | float | temperature 0.7 |
| seed | Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0) | int | seed 42 |
| stop | Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate `stop` parameters in a modelfile. | string | stop “AI assistant:“ |
| num\_predict | Maximum number of tokens to predict when generating text. (Default: -1, infinite generation) | int | num\_predict 42 |
| top\_k | Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40) | int | top\_k 40 |
| top\_p | Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9) | float | top\_p 0.9 |
| min\_p | Alternative to the top*p, and aims to ensure a balance of quality and variety. The parameter \_p* represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with *p*=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0) | float | min\_p 0.05 |


### [​](#template)TEMPLATE


`TEMPLATE` of the full prompt template to be passed into the model. It may include (optionally) a system message, a user’s message and the response from the model. Note: syntax may be model specific. Templates use Go [template syntax](https://pkg.go.dev/text/template).
#### [​](#template-variables)Template Variables




| Variable | Description |
| --- | --- |
| `{{ .System }}` | The system message used to specify custom behavior. |
| `{{ .Prompt }}` | The user prompt message. |
| `{{ .Response }}` | The response from the model. When generating a response, text after this variable is omitted. |


Copy
```
TEMPLATE """{{ if .System }}<|im\_start|>system
{{ .System }}<|im\_end|>
{{ end }}{{ if .Prompt }}<|im\_start|>user
{{ .Prompt }}<|im\_end|>
{{ end }}<|im\_start|>assistant
"""

```

### [​](#system)SYSTEM


The `SYSTEM` instruction specifies the system message to be used in the template, if applicable.
Copy
```
SYSTEM """<system message>"""

```

### [​](#adapter)ADAPTER


The `ADAPTER` instruction specifies a fine tuned LoRA adapter that should apply to the base model. The value of the adapter should be an absolute path or a path relative to the Modelfile. The base model should be specified with a `FROM` instruction. If the base model is not the same as the base model that the adapter was tuned from the behaviour will be erratic.
#### [​](#safetensor-adapter)Safetensor adapter


Copy
```
ADAPTER <path to safetensor adapter>

```

Currently supported Safetensor adapters:
* Llama (including Llama 2, Llama 3, and Llama 3.1)
* Mistral (including Mistral 1, Mistral 2, and Mixtral)
* Gemma (including Gemma 1 and Gemma 2)


#### [​](#gguf-adapter)GGUF adapter


Copy
```
ADAPTER ./ollama-lora.gguf

```

### [​](#license)LICENSE


The `LICENSE` instruction allows you to specify the legal license under which the model used with this Modelfile is shared or distributed.
Copy
```
LICENSE """
<license text>
"""

```

### [​](#message)MESSAGE


The `MESSAGE` instruction allows you to specify a message history for the model to use when responding. Use multiple iterations of the MESSAGE command to build up a conversation which will guide the model to answer in a similar way.
Copy
```
MESSAGE <role> <message>

```

#### [​](#valid-roles)Valid roles




| Role | Description |
| --- | --- |
| system | Alternate way of providing the SYSTEM message for the model. |
| user | An example message of what the user could have asked. |
| assistant | An example message of how the model should respond. |


#### [​](#example-conversation)Example conversation


Copy
```
MESSAGE user Is Toronto in Canada?
MESSAGE assistant yes
MESSAGE user Is Sacramento in Canada?
MESSAGE assistant no
MESSAGE user Is Ontario in Canada?
MESSAGE assistant yes

```

## [​](#notes)Notes


* the **`Modelfile` is not case sensitive**. In the examples, uppercase instructions are used to make it easier to distinguish it from arguments.
* Instructions can be in any order. In the examples, the `FROM` instruction is first to keep it easily readable.
[Previous](/cli)[Context lengthNext](/context-length)⌘IOn this page* [Table of Contents](#table-of-contents)
* [Format](#format)
* [Examples](#examples)
* [Basic Modelfile](#basic-modelfile)
* [Instructions](#instructions)
* [FROM (Required)](#from-required)
* [Build from existing model](#build-from-existing-model)
* [Build from a Safetensors model](#build-from-a-safetensors-model)
* [Build from a GGUF file](#build-from-a-gguf-file)
* [PARAMETER](#parameter)
* [Valid Parameters and Values](#valid-parameters-and-values)
* [TEMPLATE](#template)
* [Template Variables](#template-variables)
* [SYSTEM](#system)
* [ADAPTER](#adapter)
* [Safetensor adapter](#safetensor-adapter)
* [GGUF adapter](#gguf-adapter)
* [LICENSE](#license)
* [MESSAGE](#message)
* [Valid roles](#valid-roles)
* [Example conversation](#example-conversation)
* [Notes](#notes)